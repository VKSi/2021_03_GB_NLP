{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5-task1-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_16J0ER8WOJx",
        "nbE8Hvs7ij7o"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VKSi/2021_03_GB_NLP/blob/main/HW5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpcHsV8RWHA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQBOJRARev7"
      },
      "source": [
        "**Написать теггер на данных с руским языком**\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n",
        "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n",
        "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n",
        "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n",
        "5. сравнить все реализованные методы сделать выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16J0ER8WOJx"
      },
      "source": [
        "### загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx32O0cj9bU9"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRx8Cu_RDY1",
        "outputId": "f1bb4b0f-71ce-407f-fba7-1dd43d424daa"
      },
      "source": [
        "!pip install pyconll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgL-33mWUyZ"
      },
      "source": [
        "import pyconll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxwW9NzW570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad8949b-9db0-4d77-f242-9808a3fea7b5"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwgA3svWiRw",
        "outputId": "6bfa3687-54b3-4086-8b8f-f82eb910948b"
      },
      "source": [
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 09:49:11--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81043533 (77M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  77.29M  95.2MB/s    in 0.8s    \n",
            "\n",
            "2021-04-18 09:49:15 (95.2 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81043533/81043533]\n",
            "\n",
            "--2021-04-18 09:49:15--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10903424 (10M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  10.40M  31.8MB/s    in 0.3s    \n",
            "\n",
            "2021-04-18 09:49:17 (31.8 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10903424/10903424]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oymo30RBWjjl"
      },
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBzFe82cXGNK",
        "outputId": "98545c07-975c-4d8d-83ed-68fd8f92e7a6"
      },
      "source": [
        "for sent in full_train[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Анкета NOUN\n",
            ". PUNCT\n",
            "\n",
            "Начальник NOUN\n",
            "областного ADJ\n",
            "управления NOUN\n",
            "связи NOUN\n",
            "Семен PROPN\n",
            "Еремеевич PROPN\n",
            "был AUX\n",
            "человек NOUN\n",
            "простой ADJ\n",
            ", PUNCT\n",
            "приходил VERB\n",
            "на ADP\n",
            "работу NOUN\n",
            "всегда ADV\n",
            "вовремя ADV\n",
            ", PUNCT\n",
            "здоровался VERB\n",
            "с ADP\n",
            "секретаршей NOUN\n",
            "за ADP\n",
            "руку NOUN\n",
            "и CCONJ\n",
            "иногда ADV\n",
            "даже PART\n",
            "писал VERB\n",
            "в ADP\n",
            "стенгазету NOUN\n",
            "заметки NOUN\n",
            "под ADP\n",
            "псевдонимом NOUN\n",
            "\" PUNCT\n",
            "Муха NOUN\n",
            "\" PUNCT\n",
            ". PUNCT\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHHRZHbqgcr6"
      },
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbE8Hvs7ij7o"
      },
      "source": [
        "### 1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fy0YccFhRMk"
      },
      "source": [
        "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR9kHPZmgckl",
        "outputId": "2b005bab-bf02-4a45-e415-e395ebbecbe3"
      },
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(fdata_sent_test[100]), unigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Это', 'PRON'),\n",
              " ('сочинение', 'NOUN'),\n",
              " ('известно', 'ADJ'),\n",
              " ('во', 'ADP'),\n",
              " ('многих', 'NUM'),\n",
              " ('вариантах', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('самые', 'ADJ'),\n",
              " ('ранние', 'ADJ'),\n",
              " ('из', 'ADP'),\n",
              " ('них', 'PRON'),\n",
              " ('почти', 'ADV'),\n",
              " ('на', 'ADP'),\n",
              " ('сто', 'NUM'),\n",
              " ('лет', 'NOUN'),\n",
              " ('старше', 'ADJ'),\n",
              " (')', 'PUNCT'),\n",
              " ('и', 'CCONJ'),\n",
              " ('восходит', 'VERB'),\n",
              " ('к', 'ADP'),\n",
              " ('ещё', 'ADV'),\n",
              " ('более', 'ADV'),\n",
              " ('древним', 'ADJ'),\n",
              " ('рукописям', None),\n",
              " ('XVI', 'NUM'),\n",
              " ('в', 'ADP'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8772368820139521"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD5VjzKMgchC",
        "outputId": "419150b3-41bc-4085-af5a-f50b53be7677"
      },
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(fdata_sent_test[100]), bigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Это', 'PRON'),\n",
              " ('сочинение', None),\n",
              " ('известно', None),\n",
              " ('во', 'ADP'),\n",
              " ('многих', 'NUM'),\n",
              " ('вариантах', None),\n",
              " ('(', 'PUNCT'),\n",
              " ('самые', 'ADJ'),\n",
              " ('ранние', 'ADJ'),\n",
              " ('из', 'ADP'),\n",
              " ('них', 'PRON'),\n",
              " ('почти', 'ADV'),\n",
              " ('на', 'ADP'),\n",
              " ('сто', 'NUM'),\n",
              " ('лет', 'NOUN'),\n",
              " ('старше', 'ADJ'),\n",
              " (')', 'PUNCT'),\n",
              " ('и', 'CCONJ'),\n",
              " ('восходит', None),\n",
              " ('к', 'ADP'),\n",
              " ('ещё', None),\n",
              " ('более', 'ADV'),\n",
              " ('древним', None),\n",
              " ('рукописям', None),\n",
              " ('XVI', None),\n",
              " ('в', 'ADP'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.6963232568328109"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf7RbSKphwbD",
        "outputId": "2fb241cb-f2c4-4991-858d-83648898e5c2"
      },
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(fdata_sent_test[100]), trigram_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Это', 'PRON'),\n",
              " ('сочинение', None),\n",
              " ('известно', None),\n",
              " ('во', 'ADP'),\n",
              " ('многих', None),\n",
              " ('вариантах', None),\n",
              " ('(', None),\n",
              " ('самые', None),\n",
              " ('ранние', None),\n",
              " ('из', 'ADP'),\n",
              " ('них', 'PRON'),\n",
              " ('почти', 'ADV'),\n",
              " ('на', 'ADP'),\n",
              " ('сто', None),\n",
              " ('лет', None),\n",
              " ('старше', None),\n",
              " (')', None),\n",
              " ('и', 'PART'),\n",
              " ('восходит', None),\n",
              " ('к', None),\n",
              " ('ещё', None),\n",
              " ('более', None),\n",
              " ('древним', None),\n",
              " ('рукописям', None),\n",
              " ('XVI', None),\n",
              " ('в', 'ADP'),\n",
              " ('.', None)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.24808748694099012"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYllV_s3gceG",
        "outputId": "785629d7-06a4-4ada-faf2-345e8ea5a710"
      },
      "source": [
        "bigram_bo_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
        "display(bigram_bo_tagger.tag(fdata_sent_test[100]), bigram_bo_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Это', 'PRON'),\n",
              " ('сочинение', 'NOUN'),\n",
              " ('известно', 'ADJ'),\n",
              " ('во', 'ADP'),\n",
              " ('многих', 'NUM'),\n",
              " ('вариантах', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('самые', 'ADJ'),\n",
              " ('ранние', 'ADJ'),\n",
              " ('из', 'ADP'),\n",
              " ('них', 'PRON'),\n",
              " ('почти', 'ADV'),\n",
              " ('на', 'ADP'),\n",
              " ('сто', 'NUM'),\n",
              " ('лет', 'NOUN'),\n",
              " ('старше', 'ADJ'),\n",
              " (')', 'PUNCT'),\n",
              " ('и', 'CCONJ'),\n",
              " ('восходит', 'VERB'),\n",
              " ('к', 'ADP'),\n",
              " ('ещё', 'ADV'),\n",
              " ('более', 'ADV'),\n",
              " ('древним', 'ADJ'),\n",
              " ('рукописям', None),\n",
              " ('XVI', 'NUM'),\n",
              " ('в', 'ADP'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8829996966939642"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUXnxjbVgcbl",
        "outputId": "00c6dbca-113b-40f0-9a8a-a2396e4ba851"
      },
      "source": [
        "trigram_bo_tagger = BigramTagger(fdata_train, backoff=bigram_bo_tagger)\n",
        "display(trigram_bo_tagger.tag(fdata_sent_test[100]), trigram_bo_tagger.evaluate(fdata_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Это', 'PRON'),\n",
              " ('сочинение', 'NOUN'),\n",
              " ('известно', 'ADJ'),\n",
              " ('во', 'ADP'),\n",
              " ('многих', 'NUM'),\n",
              " ('вариантах', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('самые', 'ADJ'),\n",
              " ('ранние', 'ADJ'),\n",
              " ('из', 'ADP'),\n",
              " ('них', 'PRON'),\n",
              " ('почти', 'ADV'),\n",
              " ('на', 'ADP'),\n",
              " ('сто', 'NUM'),\n",
              " ('лет', 'NOUN'),\n",
              " ('старше', 'ADJ'),\n",
              " (')', 'PUNCT'),\n",
              " ('и', 'CCONJ'),\n",
              " ('восходит', 'VERB'),\n",
              " ('к', 'ADP'),\n",
              " ('ещё', 'ADV'),\n",
              " ('более', 'ADV'),\n",
              " ('древним', 'ADJ'),\n",
              " ('рукописям', None),\n",
              " ('XVI', 'NUM'),\n",
              " ('в', 'ADP'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8829996966939642"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koBu5LsqiyDK"
      },
      "source": [
        "** Вывод **     \n",
        " Увеличение \"граммности\" по отдельности резко ухудшает качество.     \n",
        " Применени трехграмного таггера с бэкофом к предыдущим тагерам НЕ повышает кчества.     \n",
        " Лучшее качество достигнуто на биграммном тагере с обратным вызовом юниграммного"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueZsZv8Vj4GD"
      },
      "source": [
        "### 2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHwEPIvCgcZp"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0OYUvr7ydxH"
      },
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEWUm0DbgcWm"
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFi-ocUMgcUE"
      },
      "source": [
        "test_enc_labels = le.transform(test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViMOO6svgcRi",
        "outputId": "b353c1c7-6102-47e0-f547-bb5ed4662c55"
      },
      "source": [
        "le.classes_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
              "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
              "       'VERB', 'X'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVr7HCEzzHOP"
      },
      "source": [
        "def task_1_2(vectorizer, _train_tok=train_tok, _test_tok=test_tok, _train_enc_labels=train_enc_labels):\n",
        "  X_train = vectorizer.fit_transform(train_tok)\n",
        "  X_test = vectorizer.transform(test_tok)\n",
        "  lr = LogisticRegression(random_state=0)\n",
        "  lr.fit(X_train, train_enc_labels)\n",
        "  pred = lr.predict(X_test)\n",
        "  return accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49tP_-BezHMG",
        "outputId": "35224617-8578-492f-a19f-a62b9f495a7c"
      },
      "source": [
        "print(task_1_2(HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.685454790550332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suAmAZL9zHJC",
        "outputId": "4343da5d-1c41-460f-9de4-84844f46a07e"
      },
      "source": [
        "print(task_1_2(HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.288326087689145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWs3OhZCzHFm",
        "outputId": "df514e87-5479-4f92-ca04-c1dd65f2d399"
      },
      "source": [
        "print(task_1_2(HashingVectorizer(ngram_range=(1, 5), analyzer='char_wb', n_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7333265931992047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U5ezBtGzHCG",
        "outputId": "02a3ac42-67ac-4f2c-9768-af034411682c"
      },
      "source": [
        "print(task_1_2(HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2888989990900819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK73NxR7zG-r",
        "outputId": "b9269f8a-f210-49f1-a57d-f789f0ef1730"
      },
      "source": [
        "print(task_1_2(HashingVectorizer(ngram_range=(1, 2), analyzer='char_wb', n_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7702119772183467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBurDukYzG6x",
        "outputId": "fc792606-632e-4dd7-b5cb-44e41c2cefc7"
      },
      "source": [
        "print(task_1_2(CountVectorizer(ngram_range=(1, 5), analyzer='char', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7769689616823375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD2yWBlpAIMj",
        "outputId": "6e386093-5b0c-442d-86ff-f40e4059caf7"
      },
      "source": [
        "print(task_1_2(CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7803306035790112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ3KxnZ2BcoU",
        "outputId": "881910d9-07ac-48c8-df50-09d818712067"
      },
      "source": [
        "print(task_1_2(CountVectorizer(ngram_range=(1, 1), analyzer='word', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.382081353418933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWQEp7J1GFKa",
        "outputId": "041de588-6d59-4dcc-9aab-44df0572a3de"
      },
      "source": [
        "print(task_1_2(TfidfVectorizer(ngram_range=(1, 5), analyzer='char', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7747110167492333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEWE9_K5GWSK",
        "outputId": "ce01b785-085a-458a-cdf0-cd3b2d2f1b66"
      },
      "source": [
        "print(task_1_2(TfidfVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7830098068951572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6N8MDg0GWOk",
        "outputId": "a6aa851c-141e-4bfa-f6fa-27a1d03673fe"
      },
      "source": [
        "print(task_1_2(TfidfVectorizer(ngram_range=(1, 1), analyzer='word', max_features=100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.382081353418933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghjnfMumGWKZ"
      },
      "source": [
        "import scipy.sparse as sp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqW7zCtMAIJz",
        "outputId": "1c210a36-a429-4e92-c09d-c6c354a6c87d"
      },
      "source": [
        "vectorizers = [HashingVectorizer(ngram_range=(1, 2), analyzer='char_wb', n_features=100),\n",
        "               HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=100),\n",
        "               CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100),\n",
        "               CountVectorizer(ngram_range=(1, 2), analyzer='word', max_features=100),\n",
        "               TfidfVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100),\n",
        "               TfidfVectorizer(ngram_range=(1, 1), analyzer='word', max_features=100)\n",
        "               ]\n",
        "\n",
        "for i, vectorizer in enumerate(vectorizers):\n",
        "  X_tn = vectorizer.fit_transform(train_tok)\n",
        "  X_tt = vectorizer.transform(test_tok)\n",
        "  if i == 0:\n",
        "    X_train, X_test = X_tn, X_tt\n",
        "  else:\n",
        "    X_train = sp.hstack((X_train, X_tn), format='csr')\n",
        "    X_test = sp.hstack((X_test, X_tt), format='csr')\n",
        "\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.855348296431099"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqiWvGzcJGqh"
      },
      "source": [
        "### 3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvkoYxvnJGZh"
      },
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K50eMIOaJGWK"
      },
      "source": [
        "model = Word2Vec([train_tok + test_tok], min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gKofcYpJGTj",
        "outputId": "791a35f8-2e30-456b-b83a-f90bdee8b92c"
      },
      "source": [
        "model.train([train_tok + test_tok], total_examples=model.corpus_count, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 990218)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlVoHBMDeuLI"
      },
      "source": [
        "word_vectors = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB0R55yja-t3"
      },
      "source": [
        "X_train = [word_vectors[w] for w in train_tok]\n",
        "X_test = [word_vectors[w] for w in test_tok]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UEqrwNCJGMZ",
        "outputId": "4f6b4f68-ffbc-4f8c-c7cc-dc3f508f66ad"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj4tV8ytXTry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f38eda-4f39-4379-8816-69a06937bc2f"
      },
      "source": [
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5310720183331649"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AAC2AvQeerK"
      },
      "source": [
        "# model = FastText([train_tok + test_tok], min_count=1)\n",
        "# model.train([train_tok + test_tok], total_examples=model.corpus_count, epochs=1)\n",
        "# word_vectors = model.wv\n",
        "# X_train = [word_vectors[w] for w in train_tok]\n",
        "# X_test = [word_vectors[w] for w in test_tok]\n",
        "# lr = LogisticRegression(random_state=0)\n",
        "# lr.fit(X_train, train_enc_labels)\n",
        "# pred = lr.predict(X_test)\n",
        "# accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgVJwexRsftY"
      },
      "source": [
        "Модель тренировалась больше часа. Пришлось отказаться"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brz5mVikiiyv"
      },
      "source": [
        "### 4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clPZKHvfeeil",
        "outputId": "6dbb5ced-6cee-4e92-f025-ff3ee33dff0d"
      },
      "source": [
        "vectorizers = [HashingVectorizer(ngram_range=(1, 2), analyzer='char_wb', n_features=100),\n",
        "               HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=100),\n",
        "               CountVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100),\n",
        "               CountVectorizer(ngram_range=(1, 2), analyzer='word', max_features=100),\n",
        "               TfidfVectorizer(ngram_range=(1, 2), analyzer='char_wb', max_features=100),\n",
        "               TfidfVectorizer(ngram_range=(1, 1), analyzer='word', max_features=100)\n",
        "               ]\n",
        "model = Word2Vec([train_tok + test_tok], min_count=1)\n",
        "model.train([train_tok + test_tok], total_examples=model.corpus_count, epochs=1)\n",
        "word_vectors = model.wv\n",
        "X_train = [word_vectors[w] for w in train_tok]\n",
        "X_test = [word_vectors[w] for w in test_tok]\n",
        "for vectorizer in vectorizers:\n",
        "  X_tn = vectorizer.fit_transform(train_tok)\n",
        "  X_tt = vectorizer.transform(test_tok)\n",
        "  X_train = sp.hstack((X_train, X_tn), format='csr')\n",
        "  X_test = sp.hstack((X_test, X_tt), format='csr')\n",
        "\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8564856940653119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkSNeFTHxu8z"
      },
      "source": [
        "Такая модель не дала существенного улучшения качества"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0r1qu_PxUHU"
      },
      "source": [
        "Попробуем другой вариант"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7fuO7FdwO5C"
      },
      "source": [
        "model = Word2Vec([train_tok + test_tok], min_count=1)\n",
        "model.train([train_tok + test_tok], total_examples=model.corpus_count, epochs=1)\n",
        "word_vectors = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMdsQ_JiwOlq"
      },
      "source": [
        "def w2w_ngram(n, _word_vectors, _train_tok=train_tok, _test_tok=test_tok):\n",
        "  output = [sp.csr_matrix([]), sp.csr_matrix([])]\n",
        "  for t, text in enumerate([_train_tok, _test_tok]):\n",
        "    for i, w in enumerate(text):\n",
        "      X = []\n",
        "      # emb_0 = sp.csr_matrix(list(_word_vectors[text[0]]))\n",
        "      # emb_l = sp.csr_matrix(list(_word_vectors[text[-1]]))\n",
        "      emb_0 = list(_word_vectors[text[0]])\n",
        "      emb_l = list(_word_vectors[text[-1]])\n",
        "      for j in range(i - n//2, i - n//2 + n):\n",
        "        if j < 0:\n",
        "          emb = emb_0\n",
        "        elif j >= len(text):\n",
        "          emb = emb_l\n",
        "        else:\n",
        "          emb = list(_word_vectors[w])\n",
        "        # X = sp.hstack((X, emb), format='csr')\n",
        "        X.extend(emb)\n",
        "      if i == 0:\n",
        "        output[t] = sp.csr_matrix(X)\n",
        "      else:\n",
        "        output[t] = sp.vstack((output[t], X), format='csr')\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKOKUZyu1g2B"
      },
      "source": [
        "X_train, X_test = w2w_ngram(n=2, _word_vectors=word_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "419m5yTAw8uN"
      },
      "source": [
        "К сажалению расчет занимает больше полутора часа, выполнить не удалось.\n",
        "Очень хотелось бы получить фидбэк как нужно было выполнять это задание."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHjwqXtkwukp"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA16koffwub1"
      },
      "source": [
        "X_train, X_test = w2w_ngram(n=3, _word_vectors=word_vectors)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30jIr2_7464A"
      },
      "source": [
        "X_train, X_test = w2w_ngram(n=4, _word_vectors=word_vectors)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av1tCIT85UwT"
      },
      "source": [
        "X_train, X_test = w2w_ngram(n=5, _word_vectors=word_vectors)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)\n",
        "pred = lr.predict(X_test)\n",
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}